# Notes from Blaise

In my forthcoming book [_What Is Intelligence_](https://mitpress.mit.edu/9780262049955/what-is-intelligence/)? ([first chapters online](https://whatisintelligence.antikythera.org/)) I make a case for consciousness as an evolved computational capacity grounded in social intelligence. Recent theoretical and experimental work from the Paradigms of Intelligence team at Google has begun to bolster this hypothesis more rigorously.

First, here are some things consciousness is _not_, according to my view:

1. **It is not epiphenomenal.** [^1] Some theories of consciousness hold that it exists independently of the brain, or that it “emerges” from the operation of the brain, like eldritch heat off a CPU—thus purporting to explain why stopping the brain from working normally can alter or interrupt consciousness, while leaving open the possibility of a “cold CPU” that behaves as we do, but does not “emanate” consciousness: a “philosophical zombie.”[^2] I don’t think that (convincing) philosophical zombies are a thing, for much the same reason that Good Old-Fashioned AI (e.g., ELIZA[^3]) is not convincing. Incidentally: as Michael Graziano has pointed out,[^4] the “emergent” view has a real challenge accounting for the “downward arrow”: how consciousness doesn’t just arise from the brain, but comes back around to _affect_ the brain. If it does not affect the brain (i.e., is purely epiphenomenal), then when you are asked whether you are conscious, and reflect on this for a moment before answering, we must believe that the conscious _you_ is in no way involved in those words coming out of your mouth. If, on the other hand, your consciousness _is_ involved, we need the “heat” rising off the chip to somehow flip bits on the chip too… making the whole thing start to seem rather like Descartes’s “fountaineer” puppeting the body via the soul–brain interface of the pineal gland.[^5]

2. **It is not uniquely human.** In my account, we evolved consciousness because it is necessary and integral to our behavior, and to the behavior of any highly intelligent, social, and especially cooperative agent; it is neither an evolutionary accident[^6] nor unique to our species.

3. **It is not illusory.**[^7] There is something gaslight-y about the claim that consciousness is not “real”; like saying that birds don’t exist, or chairs are not real. Consciousness is something we all experience. It certainly is a “folk theory,”[^8] and we have many pre-scientific, inconsistent, and even misleading intuitions about how it works.[^9] Our folk theories are “stagnant” in the sense that all known societies have longstanding ossified concepts of consciousness or “ensoulment,” though many disagree on which (if any) non-human entities are conscious. In these respects consciousness _is_ like heat: our understanding of it is pre-scientific, as the understanding of temperature was before the 19th century, and this can lead to confusion or wrong assumptions. But all cultures (and non-human animals, too) have _some_ understanding of temperature because it is an important latent variable in our _umwelt_, our “universe of the behaviorally meaningful.”[^10] In social animals, the same holds for consciousness. Behaviorally relevant latent variables _are_ reality.[^11]

4. **It is not supernatural.**[^12] In my account of consciousness, no extra laws of physics, new substances, fields, or energies are required.

5. **It is not (just) a quantity.** Neither is there any need to posit the sort of panpsychism that claims electrons are “a little bit” conscious—though panpsychists are correct to claim that consciousness is not a binary variable, and that “more conscious” entities can be made out of “less conscious” ones. Indeed, that is how the brain works. However, defining consciousness as an aggregative scalar quantity,[^13] like electric charge, is misleading. Computational power _can_ be quantified and _does_ scale up with parallelism, so for instance the visual and auditory cortices are indeed working in parallel and their total computational capacity can be said to be additive. However, the reason they can be “more conscious” jointly has less to do with how much computation each is doing than with their division of labor: one specializes in sound, the other in vision. Hence when these brain areas model their respective inputs (and also model each other), the aggregate can be _conscious_ of sound, visual stimuli, and the correlations between them, which is “more” than being conscious of vision alone or sound alone—but not in any simple additive or scalar sense. We also have brain areas that specialize in theory of mind for social modeling and so-called “executive function,” that is, modeling ourselves, which is why we are conscious of selves and others as well as sights and sounds (though importantly, our brain’s division of labor—like society’s—is not absolute; every part is also to some degree a generalist, which is likely essential for mutual modeling among the parts).

6. **It is not (only) material.** This may seem surprising given the above; however, biology and technology—purposive systems broadly—are rife with non-materialism. In particular, _functionality_ is non-material. When we say something is a pump, for instance, that is a functional statement. It doesn’t tell us what material that something is made of, nor even how that material is organized. Instead, it tells us what that something _does_, and what it’s _for_. (Yes, this is a kind of teleology.[^14]) The “does” part at minimum is testable by taking it for a spin (or simulating, reasoning about, or modeling it). That, in turn, requires interaction between the thing and something else that is _also_ purposive (and evaluative); hence functionality is not immanent or inherent in the way the atoms of an object are arranged—as any archaeologist who has puzzled over an artifact can attest—toy or firestarter? Clothes fastener or jewelry? Planting stick or dildo? So, this is not the “spooky non-materialism” of anti-materialists like Frank Jackson.[^15] It is, instead, the functionalism of Alan Turing, whose definition of computation is all about multiple realizability: implementational dependence on the operation of, yet functional independence from, the computational substrate.

“Souls,” then—by which I simply mean “conscious entities”—are perfectly real, but not entirely physical. They are embodied or implemented by ordinary matter subject to the usual physical laws, but are characterized by their function, which can “run” on any suitable substrate.[^16]

Consciousness is something our nervous system does, just as filtering urea from blood is something a kidney does. Kidneys evolved because the body produces urea, and urea is toxic; put another way, kidneys needed to co-evolve with the parts of the body that produce excess urea and the parts that can’t tolerate that excess. The filtering can be done by a lump of meat or by a dialysis machine (hence platform independence or multiple realizability). If it’s not done one way or another, we die.

Evolution is in general co-evolution; everything depends everything else, and more fundamentally, everything’s function is brought into being by, defined by, and continually tested by the other functions it “interfaces” with. There is something profoundly relativistic and locally interactional about this view, as it implies that there is no God’s-eye view of functionality (or, indeed, of reality) that is _not_ interactional.[^17] One person’s planting stick may be another’s dildo!

Consciousness, then, is just as real as a “chair,” a “bed,” or the above-mentioned objects. Chairs and beds are made (or in a broad sense, co-evolved with us) to carry out functions. We recognize them and interact with them functionally. But that doesn’t mean that there are special chair atoms, or that each chair atom is a little bit chair-ish, or that chair-ness is somehow immanent in those atoms. When we point and say something is a “chair” we are, in a way, invoking something immaterial and non-physical, a bit like a soul—but without the spooky.

What is different about souls vs. chairs is that the chair is probably not staring back at you. A neural net in our heads has modeled “chair,” but most of us believe the chair is not, in turn, computing any such model of us.[^18]

And computation is something brains do. They don’t filter urea, or cool the blood.[^19] They do synthesize some hormones, but mostly they process information, that is, they compute.[^20] We know this because the brain’s inputs and outputs involve transducing light, touch, sound, movement, etc. into standardized electrical pulses,[^21] which are then subject to massive computation (when we do this with artificial neural nets, we get a sense of just how computationally heavy the transformations are), then transduced back into muscle movement and other outputs.

Field potentials and chemical signals are also involved, of course. But these just amount to additional information channels with different spatiotemporal dynamics. Like an alphabet, the chemicals can vary if the senders and receivers have coevolved to “agree” on them (e.g., octopamine in invertebrates vs. norepinephrine in vertebrates). Again: multiple realizability. Any and all of these signals and the transformations among them are biophysically modelable. Hence, they are informational and computational. They can be implemented on any computationally capable substrate. That is the entire premise of computational neuroscience, which is a reasonably mature field and has made a lot of progress in understanding how the brain works. The fact that AI works reasonably well is further evidence.

So, what is the computational function of consciousness? Why do we need it, and why does it feel the way it does? The short answer is that without a sense of self, our ability to act as intelligent agents in the world is greatly compromised. When we act in a way that is informed not only by a prediction of the world out there, but of ourselves, our future needs, feelings, thoughts, and behaviors, we behave more intelligently and adaptively. We are able to make plans for the future.

Bluntly, we need to know something about “what it is like to be”[^22] ourselves in order to do a good job of staying alive. We are even able to entertain different possible counterfactual futures, while thinking about (that is, modeling) how we would be/feel in those different futures, and that is of course also highly beneficial to our survival. Free will, and our sense of ourselves as agents able to choose among alternatives, is thus part of this account.

Tellingly, though, our model of ourselves is very limited. Staying alive requires the heart to keep beating, so there’s no reason to have evolved any continual consciousness about it or agency about whether to keep it going or not—and in fact, there’s every reason for evolution to keep such choices out of our grubby little fingers. Relatedly, maintaining the right blood pressure in various parts of our bodies (constricting blood vessels as we stand up, for instance, to maintain blood flow to the brain) involves complicated self-modeling, and the parts of our nervous systems that enact this control need to be “aware of” our conscious choices just before we act on them—but our conscious selves have no reciprocal need-to-know about these physiological doings.[^23]

Of course, it’s hard to describe the situation accurately using familiar language. The foregoing explanation plays into a Cartesian folk notion that our bodies consist of an indivisible conscious homunculus embedded within an otherwise unconscious “animal-machine” (Descartes’s bête-machine), but that is not really so. Consider blindsight, the discovery by Larry Weiskrantz and Nicholas Humphrey that people can still carry out visual tasks using a subcortical pathway even when their entire visual cortex is destroyed—yet report being unaware that they can do so, insisting when carrying such tasks out that they are just guessing at random.[^24] Humphrey concludes that consciousness resides in the cortex, that blindsight is unconscious, and therefore that animals without a cerebral cortex (such as amphibians and reptiles), which rely purely on these older pathways, lack consciousness.[^25] A more sensible explanation is that the cortical regions that specialize in producing speech are wired to the visual cortex, but not to the subcortical visual pathway—so the part of you that reports on your experience simply doesn’t have the relevant information. But both parts of your brain are “you,” and both are likely conscious, albeit of different things and perhaps to varying degrees.

We have been conducting multi-agent reinforcement learning experiments showing that social games, such the prisoner’s dilemma—where it is notoriously hard for conventional RL agents to learn cooperation—are readily solved by agents that explicitly model their own future as well as that of the other player, with the default assumption that “that other player is like me.” In other words, such social and self-aware players both know what it is like to be themselves, and what it is like to be the other player—and the second both requires and uses the same cognitive machinery as the first! Such models are not naïve. They are perfectly able to change their assumptions and respond intelligently to defectors. Their starting point, however, allows them to generalize from a model of their first-person experience to that of an other mind.

It is well understood that empathy—feeling, at least to a degree, what others feel—is key to cooperation and therefore to sociality, especially among mammals like us that must care for each others’ young,[^26] cooperate with each other in large groups,[^27] teach and learn, enact division of labor, etc. That means “ensouling” those “significant others.” (Conversely, it is well documented that killing others tends to require dehumanization: a mental act of othering that amounts to “de-souling.”)

It is entirely possible that mind-modeling first arose due to the requirements of social cooperation, with self-consciousness (modeling one’s own mind) secondary[^28]—though per above, both are useful, and both appear to use the same machinery. Either way, social intelligence explosions—as have occurred among hominins and in several other animal lineages, such as cetaceans and the cleverer birds—involve feedback loops in which building stronger models of others makes one harder to model, resulting in a sort of social one-upmanship, as well as increasing group fitness. If nobody cares about you enough to model you when you are as socially dependent as humans are, you will die just as surely as if your kidneys fail—barring state or institutional intervention, which is a bit like a social dialysis machine.

The “strange loop” feeling of consciousness[^29] is not surprising, because “theory of mind” (i.e., understanding a mind’s thoughts and beliefs) is an inherently recursive act. We don’t only carry it out to first order; we imagine what someone thinks we think they think etc., up to sixth or seventh order.[^30] When we stare into the mirror we can think “what we think we think…” and that is of course weird, a bit like pointing a webcam at your own screen.

None of this seems so “hard” or non-obvious, when one thinks about it. Perhaps modern Western philosophy has gotten fixated on consciousness as a “hard” problem because the field has been so relentlessly focused on the solitary, bellybutton-gazing “cogito ergo sum” individual, or because of an insistence that properties like consciousness are objective, entailing a “view from nowhere.” One might insert a feminist critique here, or point out how recently care ethics has become a serious area of study.

On a final note, we observe theory-of-mind functionality in LLMs.[^31] This is also unsurprising, because in order for them to predict our behavior (which is of course what they are pretrained to do) they need to functionally reproduce the same computations we carry out in our brains—just as, with measures like BrainScore, we see them functionally reproducing perceptual and language computations we also carry out in our brains.[^32] Seen another way, this is just another instance of a social intelligence explosion: how could a “friendly and helpful assistant” be helpful and friendly (therefore, be “cared for” by being supplied with the electricity to run, and engaged with socially) if it could not effectively model the people it interacts with?

[^1]: Huxley 1874.
[^2]: Kirk et al. 1974.
[^3]: Weizenbaum 1966.
[^4]: Graziano 2024.
[^5]: Descartes 1677.
[^6]: Watts 2006.
[^7]: Frankish et al. 2017.
[^8]: Churchland 1981.
[^9]: For instance: the unitariness or indivisibility of one person’s consciousness, which is belied by split-brain experiments; post-hoc beliefs about when we become conscious of something and whether some action was consciously willed; phenomena highlighted by choice blindness (Johansson et al. 2005), inattentional blindness (Simons and Chabris 1999), blindsight (Humphrey 1972), and many other interesting findings.
[^10]: von Uexküll 2013 [1934].
[^11]: Weaker versions of “illusionism” make an analogy to the UIs of devices as a “user illusion” (Nørretranders 1999; Dennett 2017) but in my view there is nothing illusory about a UI, aside from skeuomorphic _trompe l’oeil_ effects like drop shadows. UIs are simply interfaces.
[^12]: Hart 2024.
[^13]: Tononi 2008.
[^14]: Rosenblueth, Wiener, and Bigelow 1943.
[^15]: Jackson 1982.
[^16]: “Swing” in a rowing crew is analogous. When a crew are working together (or _functioning_) perfectly, they experience this phenomenon, describing the feeling as one-ness, of the boat as a whole having its own agency. Swing is perfectly real and coincides with increased racing performance, but does not reside in any single rower, or any one place. It relies on the physics of the boat and of the rowers, and has real physical effects, but it is not itself physical.
[^17]: Not unlike Relational Quantum Mechanics (RQM); Rovelli 2021.
[^18]:  Caveat: per the perspective-dependence earlier, even this can be opened up for debate. Does a chair model a butt? In a sense, it does embody such a model, due to co-evolution. What is less debatable, though, is that the physics of an ordinary chair in isolation don’t support computation as defined by Turing, so there is no real-time modeling taking place locally.
[^19]: Aristotle 1882.
[^20]: Abu-Mostafa 2020.
[^21]: Adrian 1928.
[^22]: Nagel 1974.
[^23]: Breathing is an interesting intermediate case, since we also need to keep doing that, but occasionally need a degree of conscious control, as when diving, swimming, or avoiding a noxious odor.
[^24]: Humphrey and Weiskrantz 1967; Weiskrantz 1990.
[^25]: Humphrey 2023.
[^26]: Hrdy 2009.
[^27]: Humphrey 1976; Dunbar 1998; Henrich 2015.
[^28]: Graziano 2013.
[^29]: Hofstadter 2007.
[^30]: Dunbar 2016.
[^31]: Street et al. 2024.
[^32]: Schrimpf et al. 2018; Hosseini et al. 2024.
