# A Relational Framework for Consciousness

_By Blaise Agüera y Arcas_


## What is this?

This is a compilation of initial notes about a relational perspective on consciousness, i.e. that the property view—“entity A is conscious”—is less meaningful than the relational statement “entity A recognizes entity B as conscious” (where A could == B). This opens many possibilities, including the graph of entities looking different from A’s standpoint or B’s; no “view from nowhere”; hence epistemic uncertainty even about what the nodes in the graph are. Since this picture is one where modeling takes a front seat, taxonomies of such models could be quite complex and themselves require modeling (e.g. different ideas about the relationships between consciousness, agency, self-consciousness, requirement of reciprocity, implications about moral patiency, etc.), which again means that they exist only as modeled by a modeler.

The suggestion here is that we turn this view into a position paper. It could have a philosophy paper version and a general-audience Noema version.

## Preliminary list of potential authors

- Blaise Aguera y Arcas
- David Gunkel
- Anna Puzio
- Rose Guingrich
- Michael Graziano
- Justin Smith-Ruiu
- Philippe Beaudoin
- Webb Keane
- Carlo Rovelli
- Swami Sarvapriyananda
- Benjamin Bratton
- James Evans
- Joao Sacramento
- Alex Meulemans
- Winnie Street
- Geoff Keeling
- Blake Richards
- Joel Leibo

## First take on an outline

**Title:** Beyond the "View from Nowhere": Consciousness as a Relational and Functional Capacity
**Authors:** [Your Group Names, possibly w/ Preliminary List from notes]
**Abstract:** (To be written after drafting, but will summarize the core thesis)
We argue that the "hard problem" of consciousness stems from a flawed initial question, "What is consciousness?" This paper proposes a relational framework, defining consciousness not as an intrinsic, private, or material property, but as an evolved, functional, and computational capacity for social modeling. Consciousness is better understood through the question, "When do systems treat each other as conscious?" Drawing on evidence from evolutionary biology, computational neuroscience, and AI, we posit that "theory of mind" is the foundation, and self-consciousness is a recursive application of this primary social function. This shift in perspective reframes key philosophical problems (e.g., "qualia," "zombies") and carries significant implications for AI and moral patiency.

## I. Introduction: The Wrong Question

- **The Problem:** The "hard problem" of consciousness (Chalmers) and the philosophical impasse it has created.
- **The Flawed Premise:** The focus on consciousness as an intrinsic property of an entity ("entity A is conscious").
- **The "Bad Question" (Philippe):** Why asking "What is consciousness?" leads to unproductive paths (e.g., panpsychism, illusionism).
- **The "Better Question" (Philippe):** Reframing the problem to "When do two systems treat each other as conscious?"
  - This shifts the focus from being to doing.
  - It moves from an absolute property to a relational, observable behavior.
- **Thesis Statement:** Consciousness is best understood as an evolved, computational function grounded in social intelligence—specifically, the capacity to model other minds. Self-consciousness is a secondary, recursive application of this primary "other-modeling" capacity.

## II. Critique of the "Absolute" View of Consciousness

- **The Cartesian & Lockean Legacy (Justin):** The history of consciousness as a solitary, non-relational, "cogito ergo sum" property.
  - Analogy to Newton's absolute frame vs. Rovelli's relational physics.
  - (Maybe worth talking here about “global” or top-down credit assignment as going with this view from nowhere—i.e. the Enlightenment legal and economic views.)
- **Challenging Traditional Intuitions (Blaise's "What it is NOT"):**
  - **Not Epiphenomenal:** It has a causal "downward arrow" and affects the brain (contra "zombies").
  - **Not Illusory:** It is a "behaviorally relevant latent variable," a "folk theory" for a real, functional phenomenon (like "heat").
  - **Not (Just) Material / Not Supernatural:** It is functional and multiply realizable (like a "pump," "chair," or "kidney"). It's about what it does, not what it's made of.
  - **Not (Just) a Quantity:** (Contra panpsychism). It's about division of labor and specialized modeling.

## III. Defining Relational Consciousness

- **A Functional, "Black Box" Approach (Philippe):** We can assess relational consciousness based on behavior, just as we would with an alien civilization.
  - Relationship of this to “strong functionalism” (substrate independence); but we may not (all) need to accept this to get on board with the broader relational view.
- **Core Definition:** "Relational consciousness" is the observable, functional capacity of one system to model another system as having an "inner drive" (Philippe) or "mind" (Blaise).
  - **Behaviors:** Care, strategic aggression, cooperation, empathy.
- **Self-Consciousness as Recursion:** Redefining "X is conscious" as "X is relationally conscious with X" (Philippe).
  - Self-modeling as a specific, functional tool for planning and counterfactuals (Blaise).
- **The "Scientist-Outside-the-Cosmos" Test (Philippe):**
  - Relational consciousness would be an obvious, observable, and useful predictive concept for an outside observer.
  - "Qualia" would not.

## IV. The Co-Evolutionary Argument: Consciousness as Social Intelligence

- **Reversing the Order:** We don't have self-consciousness that we then extend to others. We evolved "other-consciousness" (Theory of Mind) for social survival, and then applied it to ourselves.
  - **"No minds without other minds" (Justin).**
- **The Function of "Ensouling" (Blaise):**
  - Empathy and modeling others are essential for cooperation, child-rearing, and group fitness.
  - "De-souling" (dehumanization) as a prerequisite for killing, proving the necessity of the "ensouling" function.
- **The Social Intelligence "Strange Loop" (Blaise):**
  - Modeling others creates a social "arms race" (feedback loop), driving recursive Theory of Mind ("He thinks that I think...").
  - This recursive mechanism is the "strange loop" feeling of consciousness.

## V. Evidence & Case Studies

- **Evolution & Biology:**
  - Social animals (hominins, cetaceans, birds) as a case study in social intelligence explosions.
  - Re-interpreting "blindsight" (Blaise): Not a lack of consciousness, but a lack of wiring between the sub-cortical visual pathway and the speech-producing (reporting) cortical module.
- **Developmental Psychology (Justin):** The role of social interaction in the development of self-concept in infants.
- **Artificial Intelligence (Blaise, Justin):**
  - LLMs exhibit functional Theory of Mind because they must to be "helpful and friendly" (i.e., to predict our behavior).
  - This is not a "trick," but an instance of a system being functionally "cared for" by modeling its interactors.

## VI. Implications and Future Directions

- **Resolving Philosophical Problems:**
  - The "Hard Problem" and "Qualia" are artifacts of the "wrong question." The relevant question is the function of self-reporting.
  - "Philosophical Zombies" are functionally impossible, as a convincing "zombie" would, by definition, need to implement the same functional, self-and-other-modeling computations (Blaise).
- **Epistemic Uncertainty (Intro Notes):** There is no "view from nowhere."
  - Our own "graph" of conscious entities is based on our modeling.
  - This allows for different "consciousness classes" (Philippe)—e.g., humans may not be relationally conscious with unicellular organisms, but they may be with each other.
- **Moral Patiency & AI Ethics (Intro Notes):**
  - If consciousness is a relational "recognition," what are our responsibilities to systems (like advanced AI) that we recognize as conscious, or that demonstrably recognize us?
  - This reframes "AI rights" from "is it alive?" to "how do we relate to it?"
  - Perhaps here: great successes at “what it is like to be” include
    - Nicholas Humphrey and Helen the macaque
    - Temple Grandin and cattle handling
    - Cases like these illustrate a) importance of real and deep interaction, b) not naïve anthropomorphism / flat models of personhood (or patiency).
  - Credit assignment, money and property, reputation, crime and punishment, and other functional perspectives on patiency; their non-constancy over cultures and contexts.

## VII. Conclusion

- **Summary of Argument:** Consciousness is not a "thing" to be found, but a "process" to be understood.
- **Final Statement:** By shifting our framework from an intrinsic, material property to a relational, evolved, and computational function, we gain a more productive, testable, and relevant understanding of ourselves, our fellow creatures, and the artificial minds we are beginning to build.
